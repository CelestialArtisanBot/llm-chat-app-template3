# LLM Chat Application Template

A simple, ready-to-deploy chat application template powered by Cloudflare Workers AI. This template provides a clean starting point for building AI chat applications with streaming responses.

[![Deploy to Cloudflare](https://deploy.workers.cloudflare.com/button)](https://deploy.workers.cloudflare.com/?url=https://github.com/cloudflare/templates/tree/main/llm-chat-app-template)

<!-- dash-content-start -->

## Demo

This template demonstrates how to build an AI-powered chat interface using Cloudflare Workers AI with streaming responses. It features:

- Real-time streaming of AI responses using Server-Sent Events (SSE)
- Easy customization of models and system prompts
- Support for AI Gateway integration
- Clean, responsive UI that works on mobile and desktop

## Features

- ğŸ’¬ Simple and responsive chat interface
- âš¡ Server-Sent Events (SSE) for streaming responses
- ğŸ§  Powered by Cloudflare Workers AI LLMs
- ğŸ› ï¸ Built with TypeScript and Cloudflare Workers
- ğŸ“± Mobile-friendly design
- ğŸ”„ Maintains chat history on the client
- ğŸ” Built-in Observability logging
<!-- dash-content-end -->

## Getting Started

### Prerequisites

- [Node.js](https://nodejs.org/) (v18 or newer)
- [Wrangler CLI](https://developers.cloudflare.com/workers/wrangler/install-and-update/)
- A Cloudflare account with Workers AI access

### Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/cloudflare/templates.git
   cd templates/llm-chat-app
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Generate Worker type definitions:
   ```bash
   npm run cf-typegen
   ```

### Development

Start a local development server:

```bash
npm run dev
```

This will start a local server at http://localhost:8787.

Note: Using Workers AI accesses your Cloudflare account even during local development, which will incur usage charges.

### Deployment

Deploy to Cloudflare Workers:

```bash
npm run deploy
```

### Monitor

View real-time logs associated with any deployed Worker:

```bash
npm wrangler tail
```

## Project Structure

```
/
â”œâ”€â”€ public/             # Static assets
â”‚   â”œâ”€â”€ index.html      # Chat UI HTML
â”‚   â””â”€â”€ chat.js         # Chat UI frontend script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts        # Main Worker entry point
â”‚   â””â”€â”€ types.ts        # TypeScript type definitions
â”œâ”€â”€ test/               # Test files
â”œâ”€â”€ wrangler.jsonc      # Cloudflare Worker configuration
â”œâ”€â”€ tsconfig.json       # TypeScript configuration
â””â”€â”€ README.md           # This documentation
```

## How It Works

### Backend

The backend is built with Cloudflare Workers and uses the Workers AI platform to generate responses. The main components are:

1. **API Endpoint** (`/api/chat`): Accepts POST requests with chat messages and streams responses
2. **Streaming**: Uses Server-Sent Events (SSE) for real-time streaming of AI responses
3. **Workers AI Binding**: Connects to Cloudflare's AI service via the Workers AI binding

### Frontend

The frontend is a simple HTML/CSS/JavaScript application that:

1. Presents a chat interface
2. Sends user messages to the API
3. Processes streaming responses in real-time
4. Maintains chat history on the client side

## Customization

### Changing the Model

To use a different AI model, update the `MODEL_ID` constant in `src/index.ts`. You can find available models in the [Cloudflare Workers AI documentation](https://developers.cloudflare.com/workers-ai/models/).

### Using AI Gateway

The template includes commented code for AI Gateway integration, which provides additional capabilities like rate limiting, caching, and analytics.

To enable AI Gateway:

1. [Create an AI Gateway](https://dash.cloudflare.com/?to=/:account/ai/ai-gateway) in your Cloudflare dashboard
2. Uncomment the gateway configuration in `src/index.ts`
3. Replace `YOUR_GATEWAY_ID` with your actual AI Gateway ID
4. Configure other gateway options as needed:
   - `skipCache`: Set to `true` to bypass gateway caching
   - `cacheTtl`: Set the cache time-to-live in seconds

Learn more about [AI Gateway](https://developers.cloudflare.com/ai-gateway/).

### Modifying the System Prompt

The default system prompt can be changed by updating the `SYSTEM_PROMPT` constant in `src/index.ts`.

### Styling

The UI styling is contained in the `<style>` section of `public/index.html`. You can modify the CSS variables at the top to quickly change the color scheme.

## Resources

- [Cloudflare Workers Documentation](https://developers.cloudflare.com/workers/)
- [Cloudflare Workers AI Documentation](https://developers.cloudflare.com/workers-ai/)
- [Workers AI Models](https://developers.cloudflare.com/workers-ai/models/)

Letâ€™s upgrade your README.md to reflect the new shard-phase-ritual architecture, modular synthesis pipeline, and lore-aware dashboard. This version keeps the original Cloudflare deployment flow intact while introducing your symbolic ecosystem.

---

ğŸ§™â€â™‚ï¸ Upgraded README.md

`markdown

ğŸ§  Midnight Glyph Machine â€” LLM Chat + Ritual Dashboard

A modular, lore-aware AI dashboard built on Cloudflare Workers AI. This template expands the original LLM Chat App with symbolic shard architecture, streaming responses, and multimodal synthesis rituals.

![Deploy to Cloudflare](https://deploy.workers.cloudflare.com/?url=https://github.com/cloudflare/templates/tree/main/llm-chat-app-template)

---

ğŸ”® Demo

This dashboard blends real-time AI chat with modular glyph invocation, tracepad logging, and symbolic synthesis. It features:

- ğŸ’¬ Streaming chat interface powered by Workers AI
- ğŸ§© Modular shard system for video, audio, and lore synthesis
- ğŸ“œ Tracepad logging of all rituals and invocations
- ğŸ–¼ï¸ Image-to-video generation using Hugging Face diffusers
- ğŸ”Š Audio overlays and glyph watermarking
- ğŸ§  Dynamic shard discovery and invocation

---

ğŸ§© Features

| Feature                  | Description                                      |
|--------------------------|--------------------------------------------------|
| ğŸ’¬ Chat Interface         | Streaming AI responses via SSE                   |
| ğŸ§  Workers AI             | Powered by Cloudflareâ€™s LLMs                     |
| ğŸ§© Shard Architecture     | Modular rituals grouped by phase                 |
| ğŸ“œ Tracepad Logging       | Symbolic event tracking in tracepad.json       |
| ğŸ–¼ï¸ Image-to-Video         | Synthesis via Lightricks/LTX-Video             |
| ğŸ”Š Audio Overlay          | Postprocess rituals with ambient sound           |
| ğŸ§¬ Graphviz Lore Map      | Visual graph of shard relationships              |

---

ğŸ› ï¸ Getting Started

Prerequisites

- Node.js (v18+)
- Wrangler CLI
- A Cloudflare account with Workers AI access
- Python 3.10+ with torch, diffusers, moviepy, pillow

Installation

`bash
git clone https://github.com/cloudflare/templates.git
cd templates/llm-chat-app
npm install
npm run cf-typegen
`

Development

`bash
npm run dev
`

Local server at http://localhost:8787. AI responses stream via SSE.

Deployment

`bash
npm run deploy
`

---

ğŸ§± Project Structure

`
/
â”œâ”€â”€ public/               # Chat UI
â”‚   â”œâ”€â”€ index.html        # Chat interface
â”‚   â””â”€â”€ chat.js           # Chat logic + glyph triggers
â”œâ”€â”€ src/                  # Cloudflare Worker backend
â”‚   â”œâ”€â”€ index.ts          # Chat API + model routing
â”‚   â””â”€â”€ types.ts          # Type definitions
â”œâ”€â”€ modules/              # Ritual shards
â”‚   â”œâ”€â”€ synthesis/        # Video generation
â”‚   â”œâ”€â”€ postprocess/      # Audio overlays, glyphs
â”‚   â”œâ”€â”€ metadata/         # Tracepad logging
â”‚   â””â”€â”€ init.py       # Shard discovery engine
â”œâ”€â”€ ui/                   # Dashboard scripts
â”‚   â”œâ”€â”€ registry_view.py
â”‚   â”œâ”€â”€ ritual_invoker.py
â”‚   â”œâ”€â”€ tracepad_viewer.py
â”‚   â””â”€â”€ graphviz_map.py
â”œâ”€â”€ outputs/              # Generated media
â”œâ”€â”€ tracepad.json         # Lore event log
â””â”€â”€ README.md
`

---

ğŸ§¬ How It Works

Chat Interface

- User sends incantation â†’ /api/chat
- AI streams response via SSE
- Tracepad logs invocation
- Lore fragments injected dynamically

Shard Invocation

- Each ritual is a Python function in modules/
- Discovered via init.py
- Invoked via ui/ritual_invoker.py
- Metadata logged to tracepad.json

---

ğŸ§  Customization

Change Model

Update MODEL_ID in src/index.ts.

Add New Ritual

Create a new .py file in modules/{phase}/ and tag the function with ritual.

Visualize Shards

Run:

`bash
python ui/graphviz_map.py
`

Generates lore_map.png showing shard relationships.

---

ğŸ“š Resources

- Cloudflare Workers AI
- Diffusers Library
- Graphviz
- MoviePy

---

ğŸ§™ Philosophy

Every invocation is a ritual.  
Every module is a shard.  
Every output is a lore fragment.

Welcome to the Glyph Machine.
`

---

Would you like me to generate a symbolic glyph image (glyph.svg) to embed at the top of the README?
